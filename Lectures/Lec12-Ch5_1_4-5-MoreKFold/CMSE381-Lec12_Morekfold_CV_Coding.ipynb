{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lab: More K-Fold CV \n",
    "## CMSE 381 - Fall 2022\n",
    "## Oct 7,  2022. Lecture 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fbd25",
   "metadata": {},
   "source": [
    "# 1. Plotting test/training error vs flexibility. \n",
    "\n",
    "In this module, we are going to use the $k$-fold CV to compute an approximation to test error. First, we're going to generate a data set that is clearly non-linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed so everyone has the same numbers\n",
    "np.random.seed(42)\n",
    "\n",
    "def f(t, m1 = -7,m2 = 5, m3 = -.8, b = 6):\n",
    "    return m3 * t**3 + m2*t**2 + m1*t+b\n",
    "\n",
    "n = 300\n",
    "X_toy = np.random.uniform(0,5,n)\n",
    "y_toy = f(X_toy) + np.random.normal(0,2,n)\n",
    "\n",
    "plt.scatter(X_toy,y_toy)\n",
    "\n",
    "# Doing this so the plot isn't ugly\n",
    "X_plot = X_toy.copy()\n",
    "X_plot.sort()\n",
    "plt.plot(X_plot,f(X_plot),c = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9b1fa",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Using $k$-fold cross validation for $k=5$, set up code to learn the models for each of the polynomial models below. \n",
    "- $y = \\beta_0 + \\beta_1 X$\n",
    "- $y = \\beta_0 + \\beta_1 X + \\beta_2 X^2$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3+ \\beta_4 X^4$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3+ \\beta_4 X^4+ \\beta_5 X^5$\n",
    "- $y = \\beta_0 + \\beta_1 X+ \\beta_2 X^2+ \\beta_3 X^3+ \\beta_4 X^4+ \\beta_5 X^5+ \\beta_6 X^6$\n",
    "\n",
    "Keep track of both your training and testing error at each step. Then plot your resulting training and testing errors for each degree of the polynomial. What is the best choice of polynomial for this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305262b9",
   "metadata": {},
   "source": [
    "If you still have some time, try the following:\n",
    "- see if you can figure out the test errors for everything through a degree 10 polynomial. \n",
    "- What happens to the graph if you mess around with the coefficients of the original polynomial data set you generated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84def20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16df51",
   "metadata": {},
   "source": [
    "\n",
    "![Stop Icon](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Vienna_Convention_road_sign_B2a.svg/180px-Vienna_Convention_road_sign_B2a.svg.png)\n",
    "\n",
    "Great, you got to here! Hang out for a bit, there's more lecture before we go on to the next portion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1693",
   "metadata": {},
   "source": [
    "# 2. CV for a classification data set\n",
    "![Palmer Penguins Picture](https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png)\n",
    "\n",
    "*Artwork by @allison_horst*\n",
    "\n",
    "\n",
    "For this lab, we are going to use the <a href = \"https://allisonhorst.github.io/palmerpenguins/\">Palmer Penguins</a> data set by Allison Horst, Alison Hill, and Kristen Gorman. This data set was originally posted in R, but has helpfully been loaded as an easily readable python data set by installing the `palmerpenguins` package using `pip`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a540a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should only have to do this once:\n",
    "pip install palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it worked, this should load our dataset\n",
    "from palmerpenguins import load_penguins\n",
    "penguins = load_penguins()\n",
    "penguins.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e975a8",
   "metadata": {},
   "source": [
    "As always, when playing with a new data set, your first job is to just get a feel for what's in the data. We're going to use this data to predict species of the penguin given the other information.\n",
    "\n",
    "&#9989; **<font color=red>Questions:</font>** \n",
    "- How many penguins are in the data set? \n",
    "- What are the input variables? \n",
    "- What are the possible values of the output variable? \n",
    "- Which are categorical varaibales? Which are quantitative? \n",
    "- Are there any lines with missing data? How is missing data represented in this data set? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185d071",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b80c42",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Spoiler alert, there are penguins with missing data. Replace the `penguins` dataframe with one where you have removed all those lines. (*Hint: this should be a one line operation*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bfaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624d675",
   "metadata": {},
   "source": [
    "Our next favorite thing to do with any data set is to start trying to visualize relationships between the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc288a53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Here is another nice visualization taken from the palmerpenguins github\n",
    "g = sns.lmplot(x=\"flipper_length_mm\",\n",
    "               y=\"body_mass_g\",\n",
    "               hue=\"species\",\n",
    "               height=7,\n",
    "               data=penguins,\n",
    "               palette=['#FF8C00','#159090','#A034F0'])\n",
    "g.set_xlabels('Flipper Length')\n",
    "g.set_ylabels('Body Mass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d16bc6",
   "metadata": {},
   "source": [
    "## Step 1: Set up your arrays \n",
    "\n",
    "Ok, you have your penguins data frame.  \n",
    "- Build an array $X$ with `island` and `sex` replaced with dummy variable(s)\n",
    "- Save an array of the entries in `penguins.species` as $y$ (you can use the `name_of_series.values` command, or just make a list works too). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b558a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Feel free to make more cells, I spread this out over at least \n",
    "# 5 while I was trying to get everything up and running. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e62bd9",
   "metadata": {},
   "source": [
    "## Step 2: Run logistic regression\n",
    "\n",
    "Ok, you have your penguins data with input variables as X and we are going to predict `penguins.species`. While `scikitlearn` cannot handle input variables that are categorical (hence why we had to put in our dummy variables ourselves), it's find with a predictor variable that is. The following code will fit a logistic regression on the whole data set. Of course, you know better than to actually do this to return your results, so in a moment we will be modifying this to get $k$-fold CV test errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cec1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticmodel = LogisticRegression(max_iter = 400) # Note, I needed to up the interations\n",
    "                                                   # to get rid of a convergence warning\n",
    "logisticmodel.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19672db",
   "metadata": {},
   "source": [
    "Also here's some helpful code to remember how to get accuracy/error rates out of classification modules in `scikitlearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we can also get the error rate on the training set. \n",
    "from sklearn.metrics import accuracy_score\n",
    "yhat = logisticmodel.predict(X)\n",
    "accuracy = accuracy_score(yhat, y)\n",
    "# Note that accuracy is the percentage correct\n",
    "print('Accuracy:', accuracy)\n",
    "# so the percentage incorrect is\n",
    "print('Error:', 1-accuracy)\n",
    "\n",
    "# We can get the same info directly from the original model\n",
    "print('\\nAccuracy version 2:', logisticmodel.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b7541",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Ok, your job, should you choose to accept it, is to \n",
    "- Train a model predicing `species` from all the input variables using logistic regression. \n",
    "- Use $k$-fold cross validation to determine the test error. I would recommend using something like $k=5$ to start building your code, but you can up it to $k=10$ when you want to see better results. \n",
    "- *Hint: while I was building my version, I had to set the `max_iter` for Logistic regression pretty high to get the model to converge. However, my error results were still pretty reasonable with lower `max_iter`, ignoring the massive amount of pink warning boxes. Feel free to mess around with this parameter to see how it affects your output.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e3338d56a43a0108f5ff8ffc1915439f9812d920a0d5bf5d66e4a60c981234a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
